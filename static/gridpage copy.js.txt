// --- Global Variables ---
let currentlyScannedButton = null; // Tracks the currently highlighted button
let lastGamepadInputTime = 0; // For gamepad debounce/rate limiting
let querytype = null; // Stores the type of query (e.g., 'question', 'currentevents')
let eventtype = null; // Stores specific event type if needed
let isLLMProcessing = false; // Flag to detect if LLM query is running
const clickDebounceDelay = 300; // Debounce for button clicks (adjust as needed)
let defaultDelay = 3500; // Default auditory scan delay (ms) - Loaded from settings
let currentQuestion = null; // Stores the current question context for LLM
let scanningInterval; // Holds the interval ID for scanning
let currentButtonIndex = -1; // Tracks the index for scanning
let gamepadIndex = null; // To store the index of the connected gamepad
let gamepadPollInterval = null; // Interval ID for gamepad polling
const ANNOUNCE_RELOAD_DELAY = 2000; // Delay in ms after announce before reload (adjust as needed)
// --- NEW: Wake Word Variables ---
let wakeWordInterjection = "hey"; // Default interjection (lowercase)
let wakeWordName = "bravo";       // Default name (lowercase)
let LLMOptions = 10; // Default number of options to generate
let ScanningOff = false; // Default scanning state
let SummaryOff = false; // Default summary state
const QUESTION_TEXTAREA_ID = 'question-display'; // ID of the question textarea
const LISTENING_HIGHLIGHT_CLASS = 'highlight-listening'; // CSS class for highlighting
let activeOriginatingButtonText = null; // NEW: To store the text of the button that initiated the LLM query
let activeLLMPromptForContext = null; // Store the prompt that generated current LLM buttons

// --- NEW GLOBAL VARIABLES FOR ANNOUNCEMENT QUEUE & AUDIO CONTEXT FIX ---
let announcementQueue = [];       // Queue for sequential announcements
let isAnnouncingNow = false;      // Flag to prevent concurrent announce playback
let audioContextResumeAttempted = false; // Flag for AudioContext auto-resume helper

// --- NEW: User Management Variables ---
let currentAacUserId = null; // NEW: To store the currently selected individual AAC user ID
let firebaseIdToken = null; // NEW: To store the Firebase ID Token
const AAC_USER_ID_SESSION_KEY = "currentAacUserId"; // Key for storing selected AAC user ID
const FIREBASE_TOKEN_SESSION_KEY = "firebaseIdToken"; // Key for storing Firebase ID Token
const SELECTED_DISPLAY_NAME_SESSION_KEY = "selectedDisplayName"; // Key for storing display name
const SPEECH_HISTORY_LOCAL_STORAGE_KEY = (aacUserId) => `speechHistory_${aacUserId}`;
const CHAT_HISTORY_LOCAL_STORAGE_KEY = (aacUserId) => `chatHistory_${aacUserId}`;

// --- Server URL (MUST BE this for GCP, already in your attached file) ---
const SERVER_URL = "https://talkwithbravo.com"; 

// --- Global Audio Settings (Client-Side, as per your attached file) ---
let personalSpeakerId = localStorage.getItem('bravoPersonalSpeakerId') || 'default';
let systemSpeakerId = localStorage.getItem('bravoSystemSpeakerId') || 'default';


let currentTtsVoiceName = 'en-US-Neural2-A'; // Default voice
let currentSpeechRate = 180;                 // Default words-per-minute


// --- Utility to convert Base64 to ArrayBuffer (Needed for playing audio) ---
function base64ToArrayBuffer(base64) {
    const binaryString = atob(base64);
    const len = binaryString.length;
    const bytes = new Uint8Array(len);
    for (let i = 0; i < len; i++) {
        bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
}

// --- Core Fetch Wrapper (NEW) ---
// This function will wrap all your fetch calls to automatically add auth headers
async function authenticatedFetch(url, options = {}) {
    if (!firebaseIdToken || !currentAacUserId) {
        console.error("Authentication: Firebase ID Token or AAC User ID not found. Redirecting to login.");
        // Clear any stale session data and redirect
        sessionStorage.clear();
        window.location.href = 'auth.html';
        throw new Error("Authentication required."); // Stop execution
    }

    const headers = options.headers || {};
    headers['Authorization'] = `Bearer ${firebaseIdToken}`;
    headers['X-User-ID'] = currentAacUserId;
    options.headers = headers;

    // Add a check for 401/403 responses to trigger re-authentication
    const response = await fetch(url, options);
    if (response.status === 401 || response.status === 403) {
        console.warn(`Authentication failed (${response.status}) for ${url}. Attempting re-authentication or redirecting to login.`);
        // Invalidate token/user ID and redirect to login
        sessionStorage.clear();
        alert('Your session has expired or is invalid. Please log in again.');
        window.location.href = 'auth.html';
        throw new Error("Session expired or invalid.");
    }
    return response;
}



// --- Settings Loading ---

async function loadScanSettings() {
    try {
        // Use authenticatedFetch instead of direct fetch
        const response = await authenticatedFetch('/api/settings', {
            method: 'GET' // Explicitly define method
        });
       
        if (!response.ok) {
            const errorText = await response.text();
            console.error(`HTTP error loading settings! status: ${response.status} ${errorText}`);
            // On error, we will just use the default values we set in Step 1.
            return;
        }
       
        const settings = await response.json();

        // --- Keep all your existing settings logic ---
        // Load Scan Delay
        if (settings && typeof settings.scanDelay === 'number' && !isNaN(settings.scanDelay)) {
             defaultDelay = Math.max(100, parseInt(settings.scanDelay));
             console.log(`Auditory scan delay loaded: ${defaultDelay}ms`);
        } else {
             defaultDelay = 3500;
        }

        // Load Wake Word parts
        if (settings && typeof settings.wakeWordInterjection === 'string' && settings.wakeWordInterjection.trim()) {
            wakeWordInterjection = settings.wakeWordInterjection.trim().toLowerCase();
        } else {
            wakeWordInterjection = "hey";
        }
        if (settings && typeof settings.wakeWordName === 'string' && settings.wakeWordName.trim()) {
            wakeWordName = settings.wakeWordName.trim().toLowerCase();
        } else {
            wakeWordName = "bravo";
        }
        
        // Load LLMOptions
        if (settings && typeof settings.LLMOptions === 'number' && !isNaN(settings.LLMOptions)) {
             LLMOptions = Math.max(1, parseInt(settings.LLMOptions));
        } else {
             LLMOptions = 10;
        }

        // Load booleans
        ScanningOff = settings.ScanningOff === true;
        SummaryOff = settings.SummaryOff === true;

        // --- ADD THIS NEW LOGIC FOR TTS SETTINGS ---
        // Load Speech Rate
        if (settings && typeof settings.speech_rate === 'number' && !isNaN(settings.speech_rate)) {
            currentSpeechRate = parseInt(settings.speech_rate);
            console.log(`Speech Rate loaded: ${currentSpeechRate} WPM`);
        } else {
            console.warn("Speech Rate setting not found or invalid. Using default:", currentSpeechRate);
            // It will keep its default value of 180
        }

        // Load TTS Voice Name
        if (settings && typeof settings.selected_tts_voice_name === 'string' && settings.selected_tts_voice_name.trim()) {
            currentTtsVoiceName = settings.selected_tts_voice_name;
            console.log(`TTS Voice Name loaded: "${currentTtsVoiceName}"`);
        } else {
            console.warn("TTS Voice Name setting not found or invalid. Using default:", currentTtsVoiceName);
            // It will keep its default value of 'en-US-Neural2-A'
        }
        // --- END OF NEW LOGIC ---

   } catch (error) {
       console.error('Error fetching or parsing scan settings:', error);
       // On any fetch error, the default global variables from Step 1 will be used.
   }
}


// --- Banner and Page Title Functions (MODIFIED to use selectedDisplayName) ---
function capitalizeFirstLetter(string) {
    if (!string) return "";
    return string.charAt(0).toUpperCase() + string.slice(1).toLowerCase();
}


function setBannerAndPageTitle() {
    // This function will now be called after page data is loaded
    // and the correct page name/displayName is determined.
    const urlParams = new URLSearchParams(window.location.search);
    const pageQueryParam = urlParams.get('page');
    const bannerTitleElement = document.getElementById('dynamic-page-title');
    let displayTitle = "Home"; // Default title, will be overridden

    // Revert this block to prioritize page name
    // const selectedDisplayName = sessionStorage.getItem(SELECTED_DISPLAY_NAME_SESSION_KEY); // Remove this line
    // if (selectedDisplayName) { // Remove this if block
    //     displayTitle = selectedDisplayName;
    // } else { // Remove this else branch
        const storedBannerTitle = sessionStorage.getItem('dynamicBannerTitle');
        if (pageQueryParam) { // Prioritize page name from URL for banner
            displayTitle = capitalizeFirstLetter(pageQueryParam);
        } else if (storedBannerTitle) { // Fallback to session storage if no URL param
            displayTitle = storedBannerTitle;
            sessionStorage.removeItem('dynamicBannerTitle'); // Clear it after use
        }
    // }

    // If a specific page is loaded and has a displayName, use that.
    // This will be updated again in fetchAndDisplayPage after data is loaded.
    const loadedPageDisplayName = sessionStorage.getItem('currentPageDisplayNameForBanner'); // Temp store
    if (loadedPageDisplayName) {
        displayTitle = loadedPageDisplayName;
    } 
    if (bannerTitleElement) { bannerTitleElement.textContent = displayTitle; }
    document.title = displayTitle; // Update browser tab title
}

// --- Helper to get current page information from URL ---
function getCurrentPageInfo() {
    const params = new URLSearchParams(window.location.search);
    const pageNameFromUrl = params.get('page');
    const categoryFromUrl = params.get('category');
    const topicFromUrl = params.get('topic'); // For dynamic topics

    if (pageNameFromUrl) {
        return { name: pageNameFromUrl, type: "static" };
    } else if (categoryFromUrl) {
        // For dynamic pages like current events, the "page name" can be the category.
        return { name: categoryFromUrl, type: "category" };
    } else if (topicFromUrl) {
        return { name: topicFromUrl, type: "dynamic_topic" };
    }
    return { name: "UnknownPage", type: "unknown" }; // Fallback
}



async function initializeUserContext() {
    firebaseIdToken = sessionStorage.getItem(FIREBASE_TOKEN_SESSION_KEY);
    currentAacUserId = sessionStorage.getItem(AAC_USER_ID_SESSION_KEY);

    if (!firebaseIdToken || !currentAacUserId) {
        console.log("No Firebase ID Token or AAC User ID found in session. Redirecting to auth.html.");
        sessionStorage.clear(); // Clear potentially stale data
        window.location.href = 'auth.html';
        return false; // Indicate not ready
    }
    console.log(`User context initialized. AAC User ID: ${currentAacUserId}`);
    // Update the UI to show the active user ID if you still have that element
    // This part is being removed as user selection is handled by auth.html
    /*
    const activeUserIdElement = document.getElementById('active-user-id-value');
    if (activeUserIdElement) {
        activeUserIdElement.textContent = sessionStorage.getItem(SELECTED_DISPLAY_NAME_SESSION_KEY) || currentAacUserId;
    }*/
    return true; // Indicate ready
}



// --- User Management Functions ---
// Called to load the current user state, including UI updates and refreshing local data

// The user-id-selector, set-user-id-button, and create-user-button related UI and functions
// are removed as user selection is handled by auth.html and user_select.html.
// currentAacUserId from sessionStorage is the source of truth.


// --- DOMContentLoaded Initialization ---
document.addEventListener('DOMContentLoaded', async () => {
    const userReady = await initializeUserContext();
    if (!userReady) {
        // Redirection already handled by initializeUserContext
        return;
    }

    // 2. Load scan settings (these now use currentUserId implicitly via fetch)
     // Note: currentAacUserId is used by authenticatedFetch
    await loadScanSettings();

    // Remove the user-id-selector related UI elements if they exist
    document.getElementById('user-id-selector')?.closest('div')?.remove();

    const gridContainer = document.getElementById('gridContainer');
    const params = new URLSearchParams(window.location.search);
    let pageName = params.get('page');

    if (!pageName) {
        pageName = "home";
    }

    // Set banner title based on selected user's display name or page name
    setBannerAndPageTitle();

    try {
        const response = await authenticatedFetch('/pages', {
            method: 'GET'
        });
        if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`Failed to load pages: ${response.status} - ${errorText}`);
        }
        const userPages = await response.json();

        let pageToDisplay = userPages.find(p => p.name === pageName);

        if (!pageToDisplay) {
            console.warn(`Requested page '${pageName}' not found.`);
            const homePage = userPages.find(p => p.name === "home");
            if (homePage) {
                pageToDisplay = homePage; // Corrected variable name
                pageName = "home"; // Update pageName to reflect the fallback
                console.warn(`Defaulting to 'home' page.`);
            } else if (userPages.length > 0) {
                pageToDisplay = userPages[0];
                pageName = pageToDisplay.name; // Update pageName
                console.warn(`No 'home' page. Defaulting to first available page: '${pageName}'.`);
            } else {
                console.error(`No pages found for current user. Cannot display any content.`);
                gridContainer.innerHTML = `<p class="col-span-full text-center text-red-500">Error: No pages found.</p>`;
                return;
            }
        }

         // CRITICAL FIX: Move this line to AFTER pageToDisplay is defined and validated
        // Update banner and title with the actual page being displayed
        // Ensure pageToDisplay is not null/undefined before accessing properties
        sessionStorage.setItem('currentPageDisplayNameForBanner', pageToDisplay.displayName || capitalizeFirstLetter(pageToDisplay.name));
        setBannerAndPageTitle(); // Call again to set the correct title

        generateGrid(pageToDisplay, gridContainer);

        const optionsParam = params.get('options');
        if (optionsParam) {
            const options = decodeURIComponent(optionsParam).split('\n')
                .map((option) => option.replace(/^\d+\.\s*|\\|['"]+|^\(+\d\s*/g, '').replace('1. ', '').trim())
                .filter(Boolean);
            if (options.length > 0) {
                const optionsObjects = options.map(optText => ({ summary: optText, option: optText }));
                console.log("Generating LLM buttons from URL params:", optionsObjects);
                generateLlmButtons(optionsObjects);
            } else {
                console.warn("Options parameter found but resulted in empty options array.");
            }
        }
    } catch (error) {
        console.error('Error loading page data:', error);
        gridContainer.innerHTML = `<p class="col-span-full text-center text-red-500">Error loading page data: ${error.message}.</p>`;
    }


    // --- Setup Input Listeners ---
    setupKeyboardListener(); // Ensure called
    setupGamepadListeners(); // Ensure called

    // --- Setup Speech Recognition ---
    setupSpeechRecognition();

    // --- Add AudioContext Resume Listeners (MUST HAVE for playing audio) ---
    document.body.addEventListener('mousedown', tryResumeAudioContext, { once: true });
    document.body.addEventListener('touchstart', tryResumeAudioContext, { once: true });
    document.body.addEventListener('keydown', tryResumeAudioContext, { once: true }); // Keyboard also counts as gesture

    // --- Setup Clear History Button ---
    const clearButton = document.getElementById('clear-history');
    const speechHistory = document.getElementById('speech-history');
    if (clearButton && speechHistory) {
        clearButton.addEventListener('click', () => {
            speechHistory.value = '';
            localStorage.removeItem(SPEECH_HISTORY_LOCAL_STORAGE_KEY(currentAacUserId));
        });
    } else {
        console.error("Could not find clear-history button or speech-history textarea.");
    }
    // Initial load of user-specific speech history
    if (speechHistory) {
        const storedHistory = localStorage.getItem(SPEECH_HISTORY_LOCAL_STORAGE_KEY(currentAacUserId));
        if (storedHistory) { speechHistory.value = storedHistory; }
    }

});

// --- Debounce Function ---
function debounce(func, delay) {
    let timeoutId;
    return function(...args) {
        const context = this;
        clearTimeout(timeoutId);
        timeoutId = setTimeout(() => {
            func.apply(context, args);
        }, delay);
    };
}

// --- Grid Generation ---
function generateGrid(page, container) {
    container.innerHTML = '';

    const baseGridButtonClasses = [ 'grid-button-base', 'w-full', 'h-24', 'sm:h-28', 'md:h-32', 'text-white', 'font-semibold', 'text-sm', 'sm:text-base', 'p-2', 'rounded-lg', 'shadow-md', 'border-2', 'transition', 'duration-150', 'ease-in-out', 'transform', 'hover:-translate-y-0.5', 'flex', 'items-center', 'justify-center', 'text-center', 'cursor-pointer', 'focus:outline-none', 'focus:ring-2', 'focus:ring-offset-2'];
    const defaultGridButtonColors = [ 'bg-blue-600', 'border-blue-800', 'hover:bg-blue-700', 'hover:border-blue-900', 'focus:ring-blue-500'];

    // Ensure page.buttons is an array before filtering
    const buttonsArray = Array.isArray(page.buttons) ? page.buttons : [];
    const buttonsToDisplay = buttonsArray.filter(buttonData => 
        buttonData && buttonData.text && buttonData.text.trim() !== '' && buttonData.hidden !== true
    );
    if (buttonsToDisplay.length === 0) {
         container.innerHTML = `<p class="col-span-full text-center text-gray-500">No buttons configured.</p>`;
         return;
    }

    buttonsToDisplay.forEach(buttonData => {
        console.log(`Processing button: "${buttonData.text}", Hidden property value: [${buttonData.hidden}], Type: [${typeof buttonData.hidden}]`);
        
        const button = document.createElement('button');
        button.textContent = buttonData.text;
        button.dataset.llmQuery = buttonData.LLMQuery || '';
        button.dataset.targetPage = buttonData.targetPage || '';
        button.dataset.speechPhrase = buttonData.speechPhrase || ''; // Use text as fallback
        button.dataset.queryType = buttonData.queryType || '';
        button.className = [...baseGridButtonClasses, ...defaultGridButtonColors].join(' ');
        button.addEventListener('click', debounce(() => handleButtonClick(buttonData), clickDebounceDelay));
        container.appendChild(button);
    });

    startAuditoryScanning();
}

// --- Button Click Handling ---
async function handleButtonClick(buttonData) {
    const clickTimestamp = new Date().toISOString();
    const pageInfo = getCurrentPageInfo();

    const localQueryType = buttonData.queryType || '';
    const llmQuery = buttonData.LLMQuery || '';
    const targetPage = buttonData.targetPage || '';
    const speechPhrase = buttonData.speechPhrase || '';

    const buttonLabel = buttonData.option || buttonData.text || '';
    const buttonSummary = buttonData.summary || buttonLabel;

    let contextForLog;
    if (buttonData.isLLMGenerated) {
        contextForLog = buttonData.originalPrompt;
    } else if (llmQuery) {
        contextForLog = llmQuery;
    } else {
        activeOriginatingButtonText = null;
        contextForLog = pageInfo.name;
    }

    // --- Log Button Click for Audit (MODIFIED to use authenticatedFetch) ---
    const logData = {
        timestamp: clickTimestamp,
        page_name: pageInfo.name,
        page_context_prompt: contextForLog,
        button_text: buttonLabel,
        button_summary: buttonSummary,
        is_llm_generated: buttonData.isLLMGenerated || false,
        originating_button_text: buttonData.isLLMGenerated ? buttonData.originatingButtonText : null
    };
    try {
        console.log("Sending button click log:", logData);
        const response = await authenticatedFetch('/api/audit/log-button-click', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' }, // authenticatedFetch adds Auth and X-User-ID
            body: JSON.stringify(logData)
        });
        if (!response.ok) console.error("Failed to log button click:", response.status, await response.text());
        else console.log("Button click logged successfully:", logData);
    } catch (error) {
        console.error("Error sending button click log:", error);
    }


    // --- STEP 3: Execute the main logic (the slow part) ---
    try {
        if (llmQuery) {
            document.getElementById('loading-indicator').style.display = 'flex';
            // Case 1: Button triggers an LLM query.
            if (speechPhrase) {
                await announce(speechPhrase, "system"); // Announce while spinner is showing.
            }

            if (buttonLabel) {
                sessionStorage.setItem('dynamicBannerTitle', buttonLabel);
                setBannerAndPageTitle();
            }

            isLLMProcessing = true;
            querytype = localQueryType;
            activeOriginatingButtonText = buttonLabel;
            activeLLMPromptForContext = llmQuery;

            const llmQueryWithOptions = llmQuery.replace(/#LLMOptions/g, LLMOptions);
            currentQuestion = llmQueryWithOptions;

            const summaryInstruction = SummaryOff ?
                'The "summary" key should contain the exact same FULL text as the "option" key.' :
                'If the generated option is more than 5 words, the "summary" key should be a 3-5 word abbreviation of each option, including the exact key words from the option. If the option is 5 words or less, the "summary" key should contain the exact same FULL text as the "option" key.';

            const promptForLLM = `"${llmQueryWithOptions}". Format as a JSON list... ${summaryInstruction} ...`;
            
            const options = await getLLMResponse(promptForLLM);
            generateLlmButtons(options); // This function will restart scanning.

        } else if (localQueryType === "currentevents") {
            // Case 2: Button navigates to current events.
            if (buttonLabel) {
                sessionStorage.setItem('dynamicBannerTitle', buttonLabel);
                setBannerAndPageTitle();
                activeLLMPromptForContext = `User is viewing current events for category: ${buttonData.text.toLowerCase()}`;
                activeOriginatingButtonText = buttonLabel;
            }
            isLLMProcessing = true; 
            querytype = localQueryType; 
            eventtype = buttonData.text.toLowerCase();
            await getCurrentEvents(eventtype); // This function handles its own spinner hiding.

        } else if (targetPage) {
            // Case 3: Button is a simple navigation link.
            activeOriginatingButtonText = null;
            if (speechPhrase) {
                await announce(speechPhrase, "system");
            }
            // The spinner will show until the page reloads, which is good feedback.
            setTimeout(() => {
                window.location.href = `gridpage.html?page=${targetPage}`;
            }, defaultDelay);

        } else if (speechPhrase) {
            // Case 4: Button just speaks a phrase.
            activeOriginatingButtonText = null;
            await announce(speechPhrase, "system");
            // CRITICAL: Hide spinner and restart scanning for simple speak actions.
            document.getElementById('loading-indicator').style.display = 'none';
            startAuditoryScanning();

        } else {
            // Case 5: Button with no action, hide spinner and restart scanning.
            console.warn("Button clicked with no action defined:", buttonData);
            document.getElementById('loading-indicator').style.display = 'none';
            startAuditoryScanning();
        }
    } catch (error) {
        console.error("Error in handleButtonClick main logic:", error);
        announce("Sorry, an error occurred.", "system", false);
        document.getElementById('loading-indicator').style.display = 'none';
        startAuditoryScanning(); // Restart scanning on error
    } finally {
        // This finally block now mostly ensures the processing flag is reset.
        // Spinner management is handled within each specific logic path.
        if (isLLMProcessing) {
             isLLMProcessing = false;
        }
    }
}

// --- Chat History (MODIFIED to use authenticatedFetch) ---
async function recordChatHistory(question, response) {
    try {
        const recorded = await authenticatedFetch('/record_chat_history', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' }, // authenticatedFetch adds Auth and X-User-ID
            body: JSON.stringify({ question, response })
        });
        if (!recorded.ok) {
            throw new Error(`HTTP error! status: ${recorded.status}`);
        }
        console.log("Chat history recorded successfully.");
    } catch (error) {
        console.error("Error recording chat history:", error);
    }
}


// --- Current Events ---
async function getCurrentEvents(eventType) {
    console.log(`Workspaceing current events for type: ${eventType}`);
    document.getElementById('loading-indicator').style.display = 'flex';
    
    try { // Outer try block - KEEP THIS ONE
        // REMOVE THE INNER `try {` LINE HERE
        const response = await authenticatedFetch('/get-current-events', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' }, // authenticatedFetch adds Auth and X-User-ID
            body: JSON.stringify({ eventType }),
        });

        // Check if the HTTP request itself was successful (e.g., status 200 OK)
        if (!response.ok) {
            let errorDetails = `HTTP error! status: ${response.status}`;
            try {
                const errorText = await response.text();
                errorDetails += `, Body: ${errorText}`;
            } catch (e) {
                // Ignore error reading body if it fails
            }
            console.error("Fetch request failed:", errorDetails);
            throw new Error(errorDetails);
        }

        const optionsData = await response.json();

        console.log("Successfully received and parsed data:", optionsData);

        if (Array.isArray(optionsData)) {
            generateLlmButtons(optionsData);
        } else {
            console.error("Error: Data received from server is not an array.", optionsData);
        }

    } catch (error) { // Outer catch block - KEEP THIS ONE
        console.error('Error in getCurrentEvents function:', error);
    } finally { // Outer finally block - KEEP THIS ONE
        document.getElementById('loading-indicator').style.display = 'none'; 
    }
}

// --- LLM Interaction ---
async function getLLMResponse(prompt) {
    console.log("Sending LLM Request (Prompt length):", prompt.length);
    try {
        function prepareJsonString(str) { /* ... */ }
        const response = await authenticatedFetch('/llm', { // Use authenticatedFetch
            method: 'POST',
            headers: { 'Content-Type': 'application/json' }, // authenticatedFetch adds Auth and X-User-ID
            body: JSON.stringify({ prompt: prompt }),
        });

        if (!response.ok) {
            const eT = await response.text();
            throw new Error(`LLM HTTP error! status: ${response.status} ${eT}`);
        }

        const parsedJson = await response.json();
        console.log("LLM Response Received (Raw Parsed):", parsedJson);

        if (!Array.isArray(parsedJson)) {
            console.error("LLM response was not an array:", parsedJson);
            return [];
        }

        // --- THIS IS THE NEW, INTELLIGENT TRANSFORMATION LOGIC ---
        const transformedData = parsedJson.map(item => {
            if (!item || typeof item !== 'object') {
                return null; // Ignore invalid items in the array
            }

            const summary = item.summary;
            let option = null;

            // Find the key that is NOT 'summary' and use its value as the 'option'
            const otherKeys = Object.keys(item).filter(key => key !== 'summary');
            if (otherKeys.length > 0) {
                option = item[otherKeys[0]]; // Take the value of the first other key
            }

            // If we found a summary and an option, create a standardized object
            if (summary != null && option != null) {
                return {
                    option: String(option), // Ensure they are strings
                    summary: String(summary),
                    isLLMGenerated: true,
                    originalPrompt: activeLLMPromptForContext,
                    originatingButtonText: activeOriginatingButtonText
                };
            }

            return null; // Return null if the object is not in the expected format
        }).filter(Boolean); // The .filter(Boolean) removes any null entries

        console.log("Transformed and Validated Data:", transformedData);
        
        if (transformedData.length !== parsedJson.length) {
            console.warn(`Filtered out ${parsedJson.length - transformedData.length} malformed items from the LLM response.`);
        }
        
        return transformedData;

    } catch (error) {
        console.error("Error fetching or processing LLM Response:", error);
        const loadingIndicator = document.getElementById('loading-indicator');
        if (loadingIndicator) {
            loadingIndicator.style.display = 'none';
        }
        return [];
    }
}

// --- Speech Recognition (Keyword and Question) ---
let recognition = null;
let isSettingUpRecognition = false;
let listeningForQuestion = false; // This flag is crucial

function setupSpeechRecognition() {
    if (isSettingUpRecognition || recognition) { return; }
    isSettingUpRecognition = true;
    console.log("Setting up Keyword speech recognition...");
    const SpeechRecognitionAPI = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognitionAPI) {
        console.error("Speech Recognition API not supported."); isSettingUpRecognition = false; return;
    }
    recognition = new SpeechRecognitionAPI();
    recognition.continuous = true;
    recognition.interimResults = false;
    console.log("Keyword Recognition object created:", recognition);

    recognition.onerror = function (event) {
        console.error("Keyword Speech recognition error:", event.error, event.message);
        if (['no-speech', 'audio-capture', 'network'].includes(event.error) && !listeningForQuestion) { // Only restart if not trying to listen for a question
             console.log("Keyword recognition error, attempting restart...");
             setTimeout(() => { recognition = null; isSettingUpRecognition = false; setupSpeechRecognition(); }, 1000);
        } else { isSettingUpRecognition = false; recognition = null; }
    };

    recognition.onresult = async (event) => {
        const transcript = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();
        console.log('Keyword check - Speech recognized:', transcript);
        if (listeningForQuestion) { console.log("Ignoring keyword, currently listening for question."); return; }

        const interjectionToUse = wakeWordInterjection || "hey";
        const nameToUse = wakeWordName || "brady";
        const phraseWithSpace = `${interjectionToUse} ${nameToUse}`;
        const phraseWithComma = `${interjectionToUse}, ${nameToUse}`;
        const phraseWithCommaNoSpace = `${interjectionToUse},${nameToUse}`;

        console.log(`Checking for: "${phraseWithSpace}" OR "${phraseWithComma}" OR "${phraseWithCommaNoSpace}"`);

        if (transcript.includes(phraseWithSpace) || transcript.includes(phraseWithComma) || transcript.includes(phraseWithCommaNoSpace)) {
            console.log(`Keyword detected! ("${transcript}")`);
            stopAuditoryScanning();
            if (recognition) {
                console.log("Stopping keyword recognition...");
                try { recognition.stop(); } catch(e) { console.warn("Error stopping keyword recognition:", e); }
                recognition.onresult = null; recognition.onerror = null; recognition.onend = null; recognition = null;
                console.log("Stopped and cleared keyword recognition instance.");
            }
             isSettingUpRecognition = false;

            // *** HIGHLIGHT QUESTION TEXTAREA ***
            const questionTextarea = document.getElementById(QUESTION_TEXTAREA_ID);
            if (questionTextarea) {
                questionTextarea.classList.add(LISTENING_HIGHLIGHT_CLASS);
                questionTextarea.placeholder = "Listening for your question..."; // Update placeholder
            }

            const announcement = 'Listening for your question...';
            console.log("Calling announce for question prompt...");
            try {
                await announce(announcement, "system", false);
                console.log("Announce finished. Setting up question recognition.");
                setupQuestionRecognition(); // This will set listeningForQuestion = true
            } catch (announceError) {
                 console.error("Error during announcement:", announceError);
                 if (questionTextarea) questionTextarea.classList.remove(LISTENING_HIGHLIGHT_CLASS); // Remove highlight on error
                 setupSpeechRecognition(); // Restart keyword spotting
            }
        }
    };

    recognition.onend = () => {
        console.log("Keyword Recognition ended.");
        if (!listeningForQuestion && !isSettingUpRecognition && recognition) {
             console.log("Keyword recognition ended unexpectedly, restarting.");
             recognition = null; setTimeout(setupSpeechRecognition, 500);
        } else {
             console.log("Keyword recognition ended normally or was already being reset/stopped.");
             isSettingUpRecognition = false;
        }
    };

    try { recognition.start(); console.log("Keyword recognition started."); isSettingUpRecognition = false; }
    catch (e) { console.error("Error starting keyword recognition:", e); isSettingUpRecognition = false; recognition = null; }
}

function setupQuestionRecognition() {
    console.log("Attempting to set up question recognition...");
    const SpeechRecognitionAPI = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognitionAPI) { console.error("Speech Recognition API not supported."); announce("Sorry, I can't use speech recognition.", "system", false); return; }

    let questionRecognitionInstance = new SpeechRecognitionAPI(); // Use local instance
    questionRecognitionInstance.lang = 'en-US';
    questionRecognitionInstance.continuous = false;
    questionRecognitionInstance.interimResults = true;
    questionRecognitionInstance.maxAlternatives = 1;

    let finalTranscript = ''; let listeningTimeout; let hasProcessedResult = false; let isRestartingKeyword = false;
    const questionTextarea = document.getElementById(QUESTION_TEXTAREA_ID);

    console.log("Question Recognition Config:", { continuous: false, interimResults: true, lang: 'en-US', maxAlternatives: 1 });

    questionRecognitionInstance.onstart = () => {
        console.log("Question Recognition: Listening started...");
        finalTranscript = ''; hasProcessedResult = false;
        if (questionTextarea) {
            questionTextarea.placeholder = "Listening..."; // Ensure placeholder is set
            questionTextarea.value = "";
            questionTextarea.classList.add(LISTENING_HIGHLIGHT_CLASS); // Ensure highlight is on
        }
        listeningForQuestion = true; // Set global state
        clearTimeout(listeningTimeout);
        listeningTimeout = setTimeout(() => {
             if (listeningForQuestion && !finalTranscript && !hasProcessedResult) {
                 console.log("Question Timeout: No speech detected."); announce("I didn't hear anything. Try again?", "system", false);
                 try { questionRecognitionInstance.stop(); } catch(e){}
             }
        }, 10000);
    };

    questionRecognitionInstance.onresult = async (event) => {
        console.log("Question onresult."); if (hasProcessedResult) return; clearTimeout(listeningTimeout);
        let interimTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; ++i) {
            const transcriptPart = event.results[i][0].transcript;
            if (event.results[i].isFinal) { finalTranscript += transcriptPart; } else { interimTranscript += transcriptPart; }
        }
        const displayTranscript = finalTranscript || interimTranscript;
        if (questionTextarea) questionTextarea.value = displayTranscript.trim();

        const isFinishedUtterance = event.results[event.results.length - 1].isFinal;

        if (isFinishedUtterance && finalTranscript.trim()) {
            hasProcessedResult = true; console.log("Final Question:", finalTranscript.trim().toLowerCase());
            listeningForQuestion = false; // Set state BEFORE async

            // *** REMOVE HIGHLIGHT & SHOW LOADING ***
            if (questionTextarea) questionTextarea.classList.remove(LISTENING_HIGHLIGHT_CLASS);
            document.getElementById('loading-indicator').style.display = 'flex';

            try {
                announce("Okay, processing: " + finalTranscript.trim() + ". Give me a moment.", "system", false);
                currentQuestion = finalTranscript.trim().toLowerCase();

                const summaryInstruction = SummaryOff
                ? 'The "summary" key should contain the exact same FULL text as the "option" key.'
                : 'If the generated option is more than 5 words, the "summary" key should be a 3-5 word abbreviation of each option, including the exact key words from the option. If the option is 5 words or less, the "summary" key should contain the exact same FULL text as the "option" key.';
                activeLLMPromptForContext = currentQuestion; // Set context to just the user's question
                activeOriginatingButtonText = "Voice Input"; // Mark as voice-initiated

                const promptForLLM = `
                    Provide up to "${LLMOptions}" short, single-phrase options related to: "${currentQuestion}".
                    Do not include any introductory or concluding text.
                    Format your response as a JSON list where each item has "option" and "summary" keys.
                    The "option" key should contain the FULL option text. If the option contains a question and answer, like a joke, the option contain the question and the answer.
                    ${summaryInstruction}
                    Example: [{"option": "...", "summary": "..."}]
                `;
                document.getElementById('loading-indicator').style.display = 'flex';
                const options = await getLLMResponse(promptForLLM);
                if (Array.isArray(options) && (options.length === 0 || options.every(o => typeof o === 'object' && o !== null && 'option' in o && 'summary' in o))) {
                    querytype = "question"; generateLlmButtons(options);
                } else {
                    console.error("LLM response invalid:", options); announce("Unexpected response.", "system", false);
                    isRestartingKeyword = true; setupSpeechRecognition();
                }
            } catch (error) {
                console.error('Error processing question:', error); announce("Error processing question.", "system", false);
                isRestartingKeyword = true; setupSpeechRecognition();
            } finally {
                //document.getElementById('loading-indicator').style.display = 'none';
                document.getElementById('loading-indicator').style.display = 'none'; // Ensure indicator is hidden
                if (questionTextarea) questionTextarea.placeholder = "Ask a question..."; 
                console.log("LLM processing finished for question.");
            }
        } else if (!isFinishedUtterance) { console.log("Waiting for final result..."); }
        else { console.log("Final utterance empty."); listeningForQuestion = false; if (questionTextarea) questionTextarea.classList.remove(LISTENING_HIGHLIGHT_CLASS); }
    };

    questionRecognitionInstance.onerror = (event) => {
        clearTimeout(listeningTimeout); if (hasProcessedResult) return;
        console.error("Question Error:", event.error, event.message);
        let errorMessage = "Speech recognition error."; let attemptRetry = false;
        if (event.error === 'no-speech') {
            errorMessage = "Didn't hear anything. Try again?";
            if (!questionRecognitionInstance.hasRetried) { attemptRetry = true; errorMessage += " Retrying..."; questionRecognitionInstance.hasRetried = true; } // Use instance flag
            else { console.log("Already retried."); }
        } else if (event.error === 'not-allowed' || event.error === 'service-not-allowed') { errorMessage = "Mic access denied."; }
        else if (event.error === 'audio-capture') { errorMessage = "Mic problem."; }
        else if (event.error === 'network') { errorMessage = "Network error."; }
        else if (event.error === 'aborted') { errorMessage = ""; }

        if (errorMessage) { announce(errorMessage, "system", false); }
        document.getElementById('loading-indicator').style.display = 'none';
        if (questionTextarea) {
            questionTextarea.placeholder = "Ask a question...";
            questionTextarea.classList.remove(LISTENING_HIGHLIGHT_CLASS); // Remove highlight on error
        }
        listeningForQuestion = false;
        try { questionRecognitionInstance.stop(); } catch(e) {}

        if (attemptRetry) {
            console.log("Attempting retry...");
            setTimeout(() => {
                 try { finalTranscript = ''; listeningForQuestion = true; hasProcessedResult = false; questionRecognitionInstance.start(); }
                 catch (e) { console.error("Retry start error:", e); announce("Retry failed.", "system", false); listeningForQuestion = false; isRestartingKeyword = true; setupSpeechRecognition(); }
            }, 500);
        } else { 
            console.log("Not retrying. Restarting keyword listener.");
            isRestartingKeyword = true;
            setupSpeechRecognition();
            if (document.querySelectorAll('#gridContainer button:not([style*="display: none"])').length > 0) {
                console.log("Error in question rec (not retrying), restarting scanning for existing buttons.");
                startAuditoryScanning();
            }
        }
    };

    questionRecognitionInstance.onend = () => {
        clearTimeout(listeningTimeout); console.log("Question Recognition ended.");
        const wasRetried = questionRecognitionInstance?.hasRetried; const stillListening = listeningForQuestion; // Capture state before reset
        if (listeningForQuestion) { listeningForQuestion = false; console.log("Reset listening flag in onend."); }

        if (questionTextarea) {
            questionTextarea.classList.remove(LISTENING_HIGHLIGHT_CLASS); // Ensure highlight is removed
            questionTextarea.placeholder = "Ask a question...";
        }
        // document.getElementById('loading-indicator').style.display = 'none'; // Moved to onresult's finally block

        if (stillListening && !hasProcessedResult && !wasRetried) { console.log("Ended without result/retry."); announce("Didn't catch that. Try again?", "system", false); }

        if (!hasProcessedResult && !isRestartingKeyword) {
            console.log("Restarting keyword listener from onend.");
            setupSpeechRecognition();
            // If ending without result and falling back to keyword spotting, attempt to restart scanning.
            if (document.querySelectorAll('#gridContainer button:not([style*="display: none"])').length > 0) {
                console.log("Question rec ended (no result), restarting scanning for existing buttons.");
                startAuditoryScanning();
            }
        } else { console.log("Not restarting keyword listener from onend (processed, retrying, or handled by error)."); }

        questionRecognitionInstance = null; console.log("Question instance cleaned up.");
    };

    questionRecognitionInstance.onnomatch = () => { console.warn("Question No match."); };
    questionRecognitionInstance.onspeechend = () => {
        console.log("Question Speech ended."); clearTimeout(listeningTimeout);
        listeningTimeout = setTimeout(() => {
            if (listeningForQuestion && !hasProcessedResult) {
                console.warn("Timeout after speech end."); announce("Didn't get a final result.", "system", false);
                try { questionRecognitionInstance.stop(); } catch(e){}
            }
        }, 5000);
    };
    questionRecognitionInstance.onsoundend = () => { console.log("Question Sound ended."); };

    setTimeout(() => {
        try { console.log("Calling start() for question recognition..."); questionRecognitionInstance.start(); }
        catch (e) { console.error("Start error:", e); announce("Couldn't start listening.", "system", false); listeningForQuestion = false; clearTimeout(listeningTimeout); isRestartingKeyword = true; setupSpeechRecognition(); }
    }, 150);
}


// --- Core Audio Playback Function (Centralized audio processing) ---
// This function is called by `processAnnouncementQueue` to play synthesized audio.
async function playAudioToDevice(audioDataBuffer, sampleRate, announcementType) { // announcementType needed to determine target speaker
    console.log(`playAudioToDevice: Starting playback for type "${announcementType}"`);

    // Get speaker IDs directly here, as this is the actual playback point.
    const personalSpeakerId = localStorage.getItem('bravoPersonalSpeakerId') || 'default';
    const systemSpeakerId = localStorage.getItem('bravoSystemSpeakerId') || 'default';
    
    let targetOutputDeviceId;
    if (announcementType === 'personal') {
        targetOutputDeviceId = personalSpeakerId;
    } else if (announcementType === 'system') {
        targetOutputDeviceId = systemSpeakerId;
    } else {
        targetOutputDeviceId = 'default';
    }
    console.log(`playAudioToDevice: Final targetOutputDeviceId: "${targetOutputDeviceId}"`);

    if (!audioDataBuffer) {
        console.error('playAudioToDevice: No audio data buffer provided.');
        throw new Error('No audio data buffer provided.');
    }

    let audioContext;
    let source;

    try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        if (audioContext.state === 'suspended') {
            console.log("playAudioToDevice: AudioContext is suspended, attempting to resume.");
            // We resume here; the tryResumeAudioContext() will provide the gesture.
            audioContext.resume().catch(err => {
                console.warn("playAudioToDevice: .resume() failed, probably no user gesture yet:", err);
            });
        }

        if (typeof audioContext.setSinkId === 'function' && targetOutputDeviceId && targetOutputDeviceId !== 'default') {
            console.log(`playAudioToDevice: Attempting to call audioContext.setSinkId to device (ID: ${targetOutputDeviceId})`);
            await audioContext.setSinkId(targetOutputDeviceId);
            console.log(`playAudioToDevice: setSinkId call FINISHED for device ${targetOutputDeviceId}`);
        } else {
            console.warn(`playAudioToDevice: Not performing explicit routing. Audio will play to browser's default speaker.`);
        }

        const audioBuffer = await audioContext.decodeAudioData(audioDataBuffer);
        console.log("playAudioToDevice: Audio data decoded. Buffer duration:", audioBuffer.duration);

        source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);
        source.start(0);
        console.log("playAudioToDevice: Audio source started.");

        return new Promise((resolve) => {
            source.onended = () => {
                console.log("playAudioToDevice: Audio playback ended.");
                audioContext.close(); // Important to release resources
                resolve();
            };
        }).catch(err => {
            console.error("playAudioToDevice: Error during audio playback promise:", err);
            if (audioContext && audioContext.state !== 'closed') audioContext.close();
            throw err;
        });

    } catch (error) {
        console.error('playAudioToDevice: Fatal Error during setup or playback:', error);
        if (audioContext && audioContext.state !== 'closed') audioContext.close();
        throw error;
    }
}

// --- Announcement Queue Processor ---
// This function manages playing announcements from the queue sequentially.
async function processAnnouncementQueue() {
    if (isAnnouncingNow || announcementQueue.length === 0) {
        return; // Already playing or nothing to play.
    }

    isAnnouncingNow = true;
    const { textToAnnounce, announcementType, recordHistory, resolve, reject } = announcementQueue.shift(); 

    console.log(`ANNOUNCE QUEUE: Playing "${textToAnnounce.substring(0, 30)}..." (Type: ${announcementType})`);

    try {
        // Fetch audio data from your server using authenticatedFetch
        const response = await authenticatedFetch(`${SERVER_URL}/play-audio`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' }, // authenticatedFetch adds Auth and X-User-ID
            body: JSON.stringify({ text: textToAnnounce, routing_target: announcementType }),
        });

        if (!response.ok) {
            const errorBody = await response.json().catch(() => response.text());
            throw new Error(`Failed to synthesize audio: ${response.status} - ${JSON.stringify(errorBody)}`);
        }

        const jsonResponse = await response.json();
        const audioData = jsonResponse.audio_data;
        const sampleRate = jsonResponse.sample_rate;

        if (!audioData) {
            throw new Error("No audio data received from server.");
        }

        const audioDataArrayBuffer = base64ToArrayBuffer(audioData);
        await playAudioToDevice(audioDataArrayBuffer, sampleRate, announcementType);

        if (recordHistory) {
            const speechHistory = document.getElementById('speech-history');
            if (speechHistory) {
                let history = (localStorage.getItem(SPEECH_HISTORY_LOCAL_STORAGE_KEY(currentAacUserId)) || '').split('\n').filter(Boolean);
                history.unshift(textToAnnounce);
                if (history.length > 20) { history = history.slice(0, 20); }
                speechHistory.value = history.join('\n');
                localStorage.setItem(SPEECH_HISTORY_LOCAL_STORAGE_KEY(currentAacUserId), speechHistory.value);
            } else {
                console.warn("Speech history textarea not found for recording.");
            }
        }
        
        resolve(); 

    } catch (error) {
        console.error('ANNOUNCE QUEUE: Error during announcement playback:', error);
        reject(error);
    } finally {
        isAnnouncingNow = false;
        if (announcementQueue.length > 0) {
            processAnnouncementQueue();
        }
    }
}


// --- Announce Function (MODIFIED to use the queue) ---
// This function will now queue up messages for sequential playback.
async function announce(textToAnnounce, announcementType = "system", recordHistory = true) {
    console.log(`ANNOUNCE: QUEUING "${textToAnnounce.substring(0, 30)}..." (Type: ${announcementType})`);
    
    // Create and return a new Promise, whose resolve/reject functions are stored in the queue.
    return new Promise((resolve, reject) => {
        announcementQueue.push({
            textToAnnounce,
            announcementType,
            recordHistory,
            resolve, // Store the resolve function of this promise
            reject   // Store the reject function of this promise
        });

        // Trigger the queue processing. It will only start playing if not already playing.
        processAnnouncementQueue();
    });
}


// --- Global AudioContext Resume Helper ---
// This function tries to resume the AudioContext on first user gesture.
function tryResumeAudioContext() {
    if (window.AudioContext && !audioContextResumeAttempted) {
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        if (audioContext.state === 'suspended') {
            audioContext.resume().then(() => {
                console.log('AudioContext resumed via user gesture.');
            }).catch(e => {
                console.warn('Failed to auto-resume AudioContext on initial gesture:', e);
            });
        }
        audioContextResumeAttempted = true; // Ensure this only runs once per page load
    }
}




// --- Auditory Scanning ---
function startAuditoryScanning() {
    stopAuditoryScanning();
    if (ScanningOff) { console.log("Auditory scanning is off."); return; }
    console.log("Starting auditory scanning...");
    const buttons = Array.from(document.querySelectorAll('#gridContainer button:not([style*="display: none"])'));
    if (buttons.length === 0) { console.log("No visible buttons found."); currentlyScannedButton = null; return; }
    currentButtonIndex = -1;

    const scanStep = () => {
        if (currentlyScannedButton) { currentlyScannedButton.classList.remove('scanning'); }
        currentButtonIndex++;
        if (currentButtonIndex >= buttons.length) { currentButtonIndex = 0; }
        if (buttons[currentButtonIndex]) {
            currentlyScannedButton = buttons[currentButtonIndex];
            speakAndHighlight(currentlyScannedButton);
        } else { console.warn("Button not found at index:", currentButtonIndex); currentButtonIndex = -1; }
    };
    console.log(`Scan delay set to: ${defaultDelay}ms`);
    scanStep(); // Perform first scan immediately
    scanningInterval = setInterval(scanStep, defaultDelay);
}

async function speakAndHighlight(button) {
    document.querySelectorAll('#gridContainer button.scanning').forEach(btn => { btn.classList.remove('scanning'); });
    button.classList.add('scanning');
    try {
        const textToSpeak = button.textContent;
        const utterance = new SpeechSynthesisUtterance(textToSpeak);
        window.speechSynthesis.cancel();
        window.speechSynthesis.speak(utterance);
    } catch (e) { console.error("Speech synthesis error:", e); }
}

function stopAuditoryScanning() {
    console.log("Stopping auditory scanning.");
    clearInterval(scanningInterval); scanningInterval = null;
    if (currentlyScannedButton) { currentlyScannedButton.classList.remove('scanning'); currentlyScannedButton = null; }
    currentButtonIndex = -1; window.speechSynthesis.cancel();
}

// --- LLM Button Generation ---
function generateLlmButtons(options) {
    document.getElementById('loading-indicator').style.display = 'none';
    options = Array.isArray(options) ? options : [];
    stopAuditoryScanning();
    const gridContainer = document.getElementById('gridContainer');
    if (!gridContainer) { console.error("gridContainer not found!"); return; }
    gridContainer.innerHTML = '';
    currentOptions = options;
    let isAnnouncing = false;
    console.log("Generating buttons for options:", options);

     // --- Define Tailwind classes ---
     const baseButtonClasses = [ 'grid-button-base', 'w-full', 'h-auto', 'text-white', 'font-semibold', 'text-sm', 'sm:text-base', 'p-3', 'rounded-lg', 'shadow-md', 'border-2', 'transition', 'duration-150', 'ease-in-out', 'transform', 'hover:-translate-y-0.5', 'flex', 'items-center', 'justify-center', 'text-center', 'cursor-pointer', 'focus:outline-none', 'focus:ring-2', 'focus:ring-offset-2', 'mb-3' ];
     const llmButtonColors = [ 'bg-teal-600', 'border-teal-800', 'hover:bg-teal-700', 'hover:border-teal-900', 'focus:ring-teal-500' ];
     const specialButtonColors = [ 'bg-gray-600', 'border-gray-800', 'hover:bg-gray-700', 'hover:border-gray-900', 'focus:ring-gray-500' ];
     const askAgainButtonColors = [ 'bg-yellow-500', 'border-yellow-700', 'hover:bg-yellow-600', 'hover:border-yellow-800', 'text-black', 'focus:ring-yellow-400' ];
     const goBackButtonColors = [ 'bg-gray-200', 'border-gray-400', 'hover:bg-gray-300', 'hover:border-gray-500', 'text-black', 'focus:ring-gray-300' ];

    // --- Generate Buttons for LLM Options ---
    options.forEach(optionData => {
        if (!optionData || typeof optionData.summary !== 'string' || typeof optionData.option !== 'string') { console.warn("Skipping invalid option data:", optionData); return; }
        const button = document.createElement('button');
        button.textContent = optionData.summary;
        button.dataset.option = optionData.option;
        button.dataset.speechPhrase = optionData.option;
        button.className = [...baseButtonClasses, ...llmButtonColors].join(' ');


        // Pass the full optionData object, which now includes isLLMGenerated and originalPrompt
        button.addEventListener('click', debounce(async () => {
            if (isAnnouncing) { console.log("Announcement in progress..."); return; }
            isAnnouncing = true;
            stopAuditoryScanning(); // Stop scanning when an option is chosen
            console.log("LLM Button Click. Announcing:", optionData.option); // Log the full option text

            // --- Log LLM Button Click for Audit ---
            const clickTimestamp = new Date().toISOString();
            const pageInfo = getCurrentPageInfo();
            const logData = {
                timestamp: clickTimestamp,
                page_name: pageInfo.name,
                page_context_prompt: optionData.originalPrompt, // From tagged optionData
                button_text: optionData.option,
                button_summary: optionData.summary,
                is_llm_generated: optionData.isLLMGenerated, // Should be true
                originating_button_text: optionData.originatingButtonText // NEW: Add originating button text
            };
            console.log("Logging LLM button click data:", logData);
            try {
                const auditResponse = await fetch('/api/audit/log-button-click', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' }, // authenticatedFetch will add other headers
                    body: JSON.stringify(logData)
                });
                if (!auditResponse.ok) console.error("Failed to log LLM button click:", auditResponse.status, await auditResponse.text());
                else console.log("LLM button click logged successfully:", logData);
            } catch (auditError) {
                console.error("Error sending LLM button click log:", auditError);
            }
            // --- End Audit Logging ---
            try {
                // Announce the selected option (using the full option text for clarity)
                await announce(optionData.option, "system"); // Use optionData.option directly
                console.log("Announcement finished for:", button.dataset.option);

                // Clear the question display after announcement
                activeLLMPromptForContext = null; // Clear context after selection
                const questionDisplay = document.getElementById('question-display');
                if (questionDisplay) { questionDisplay.value = ''; }

                // Reload the page to go back to the initial state
                // Consider if this is always the desired behavior
                window.location.reload(true);

            } catch (error) {
                 console.error("Error during announcement or reload:", error);
                 // Optionally restart scanning here if announce fails
                 startAuditoryScanning();
            } finally {
                isAnnouncing = false; // Reset flag  
            }
        }, clickDebounceDelay)); // Apply debounce
        gridContainer.appendChild(button);
    });

    // --- Generate Special Buttons ---
    // (Ensure these also have the base class and appropriate color classes)
    const somethingElseButton = document.createElement('button');
    somethingElseButton.textContent = 'Something Else';
    somethingElseButton.className = [...baseButtonClasses, ...specialButtonColors].join(' ');
    somethingElseButton.addEventListener('click', async () => {
        stopAuditoryScanning(); // Stop scanning before fetching new options
        console.log("Something Else button clicked. Query Type:", querytype, "Current Question context:", currentQuestion);

       try { // Wrap main logic
            // --- Logic based on query type ---
            if (querytype === "currentevents") {
                // For "Something Else" in current events, the activeOriginatingButtonText
                // should still be the category button that initiated it.
                console.log("Getting next set of current events for type:", eventtype);
                await getCurrentEvents(eventtype); // This calls generateLlmButtons, which starts scanning
            } else if (querytype === "question" || querytype === "options") {
                document.getElementById('loading-indicator').style.display = 'flex';
                try {
                    const excludedOptionsText = currentOptions
                        .map(opt => opt.option)
                        .filter(text => typeof text === 'string' && text.trim() !== '')
                        .join("; ");
                    console.log("Excluding options:", excludedOptionsText);

                    const summaryInstruction = SummaryOff
                        ? 'The "summary" key should contain the exact same FULL text as the "option" key.'
                        : 'If the generated option is more than 5 words, the "summary" key should be a 3-5 word abbreviation of each option, including the exact key words from the option. If the option is 5 words or less, the "summary" key should contain the exact same FULL text as the "option" key.';
                        // Set activeLLMPromptForContext to the original concise question before calling getLLMResponse
                        activeLLMPromptForContext = currentQuestion;
                        // activeOriginatingButtonText should still be set from the initial question or button click
                        // that generated the current set of options.
                    const prompt = `
                        For the question "${currentQuestion}", provide new options.
                        IMPORTANTLY, exclude the following options if possible: "${excludedOptionsText}".
                        Return ONLY a numbered list of the new options. Do not include any introductory or concluding text.
                        Format your response as a JSON list where each item has "option" and "summary" keys.
                        ${summaryInstruction}
                        The "option" key should contain the FULL option text. If the option contains a question and answer, like a joke, the option contain the question and the answer.
                        Example: [{"option": "...", "summary": "..."}]
                    `;
                    console.log("Sending prompt for 'Something Else' options:", prompt);

                    const response = await getLLMResponse(prompt); // Expects an array

                    if (Array.isArray(response)) {
                        if (response.length > 0) {
                            generateLlmButtons(response); // This will restart scanning
                        } else {
                            console.warn("LLM did not return any new options after exclusion (array response).");
                            announce("Sorry, I couldn't find any other options for that.", "system", false);
                        }
                    } else if (typeof response === 'string' && response.trim() !== '') { // Fallback for older string response
                        console.warn("Received string response from getLLMResponse for 'Something Else', expected array. Processing as string.");
                        const newOptions = response.split("\n").map(option => option.replace(/^\s*\d+[\.\)]?\s*|\s*\*+\s*|["']/g, '').trim()).filter(option => option !== "");
                        if (newOptions.length > 0) {
                            const newOptionsObjects = newOptions.map(optText => ({ summary: optText, option: optText }));
                            generateLlmButtons(newOptionsObjects); // This will restart scanning
                        } else {
                            console.warn("LLM did not return any new options after exclusion (string response).");
                            announce("Sorry, I couldn't find any other options for that.", "system", false);
                        }
                    } else {
                        console.error("Unexpected or empty response type from getLLMResponse for 'Something Else':", response);
                        announce("Sorry, I received an unexpected response for more options.", "system", false);
                    }
                    } catch (error) {
                    console.error('Error getting new LLM options for "Something Else":', error);
                    announce("Sorry, an error occurred while getting more options.", "system", false);
                } finally {
                    document.getElementById('loading-indicator').style.display = 'none';
                }
                } else {
                console.warn("Something Else clicked with unknown querytype:", querytype);
                announce("Sorry, I'm not sure what to do for 'Something Else' here.", "system", false);
            }
            } finally {
            // This 'finally' ensures that scanning is attempted to be restarted
            // if it wasn't already started by generateLlmButtons (e.g., due to error or no new options).
            if (!scanningInterval) { // Check if scanning is not already active
                console.log("Attempting to restart scanning in 'Something Else' finally block.");
                // Restore activeLLMPromptForContext if needed for scanning, though it's usually for the next LLM call
                // activeLLMPromptForContext = currentQuestion; // Or a specific "something else" context
                startAuditoryScanning();
            }
        }
    });


    
    gridContainer.appendChild(somethingElseButton);

    if (querytype === "question") {
        const askAgainButton = document.createElement('button');
        askAgainButton.textContent = 'Please ask me again';
        askAgainButton.id = 'askAgainButton';
        askAgainButton.className = [...baseButtonClasses, ...askAgainButtonColors].join(' ');
        askAgainButton.addEventListener('click', () => {
            stopAuditoryScanning();
            activeOriginatingButtonText = null; // Reset, as a new question will be asked
            activeLLMPromptForContext = null;
            announce('Okay, please ask your question again after the tone.', "system", false)
                .then(() => { activeLLMPromptForContext = "User chose to ask question again."; })
                .then(() => {
                    // Reset state and start question recognition again
                    listeningForQuestion = false; // Ensure state allows starting new recognition
                    // No need to stop recognition instance here, setupQuestionRecognition creates a new one
                    setTimeout(() => {
                        setupQuestionRecognition(); // Start listening for the question
                    }, 100); // Short delay after announcement
                })
                .catch(err => {
                    console.error("Error announcing 'ask again':", err);
                    // If announce fails, revert to keyword spotting and scan existing buttons
                    setupSpeechRecognition();
                    activeLLMPromptForContext = null; // Clear context
                    if (document.querySelectorAll('#gridContainer button:not([style*="display: none"])').length > 0) {
                        startAuditoryScanning();
                    }
                });
        });
        gridContainer.appendChild(askAgainButton);
    }

    const goBackButton = document.createElement('button');
    goBackButton.textContent = 'Go Back';
    goBackButton.className = [...baseButtonClasses, ...goBackButtonColors].join(' ');
    goBackButton.addEventListener('click', () => {
        activeOriginatingButtonText = null; // Reset on navigation
        activeLLMPromptForContext = null; // Clear context on navigation
        window.location.reload(true);
    })
    gridContainer.appendChild(goBackButton);


    // --- Start Auditory Scanning ---
    if (gridContainer.childElementCount > 0) {
         console.log("Starting auditory scanning for generated LLM/special buttons.");
        // activeLLMPromptForContext is already set by the function that called generateLlmButtons
         startAuditoryScanning();
    } else { console.log("No buttons generated, not starting scanning."); }
}

// --- Input Handling --- ADDED/VERIFIED ---

/**
 * Sets up the keyboard listener for the Spacebar.
 */
function setupKeyboardListener() {
    document.addEventListener('keydown', (event) => {
        if (event.code === 'Space' && !isLLMProcessing && !listeningForQuestion && currentlyScannedButton) {
            event.preventDefault();
            const buttonToActivate = currentlyScannedButton; // Capture the button reference
            console.log("Spacebar pressed, activating button:", buttonToActivate.textContent);
            buttonToActivate.click();
            buttonToActivate.classList.add('active');
            // Use the captured reference in the timeout as well
            setTimeout(() => buttonToActivate?.classList.remove('active'), 150);
        }
    });
    console.log("Keyboard listener (Spacebar) set up.");
}

/**
 * Sets up listeners for gamepad connection/disconnection and starts polling.
 */
function setupGamepadListeners() {
    window.addEventListener("gamepadconnected", (event) => {
        console.log("Gamepad connected:", event.gamepad.index, event.gamepad.id);
        if (gamepadIndex === null) { gamepadIndex = event.gamepad.index; startGamepadPolling(); }
    });
    window.addEventListener("gamepaddisconnected", (event) => {
        console.log("Gamepad disconnected:", event.gamepad.index, event.gamepad.id);
        if (gamepadIndex === event.gamepad.index) { gamepadIndex = null; stopGamepadPolling(); }
    });
    console.log("Gamepad connection listeners set up.");
     const gamepads = navigator.getGamepads ? navigator.getGamepads() : [];
     for (let i = 0; i < gamepads.length; i++) {
         if (gamepads[i]) {
             console.log("Gamepad already connected at index:", i);
             if (gamepadIndex === null) { gamepadIndex = i; startGamepadPolling(); }
             break;
         }
     }
}

/**
 * Starts the gamepad polling loop using requestAnimationFrame.
 */
function startGamepadPolling() {
    if (gamepadPollInterval !== null) return;
    console.log("Starting gamepad polling for index:", gamepadIndex);
    let lastButtonState = false;

    function pollGamepads() {
        if (gamepadIndex === null) { stopGamepadPolling(); return; }
        const gp = navigator.getGamepads()[gamepadIndex];
        if (!gp) { gamepadPollInterval = requestAnimationFrame(pollGamepads); return; } // Keep polling if gp not ready

        const currentButtonState = gp.buttons[0] && gp.buttons[0].pressed;
        if (currentButtonState && !lastButtonState) {
             const now = Date.now();
             if (now - lastGamepadInputTime > 300) { // Rate limit
                if (!isLLMProcessing && !listeningForQuestion && currentlyScannedButton) {
                    const buttonToActivate = currentlyScannedButton; // Capture the button reference
                    console.log("Gamepad button 0 pressed, activating button:", buttonToActivate.textContent);
                    buttonToActivate.click();
                    buttonToActivate.classList.add('active');
                    // Use the captured reference in the timeout
                    setTimeout(() => buttonToActivate?.classList.remove('active'), 150);
                    lastGamepadInputTime = now;
                } else { console.log("Gamepad press ignored (state)."); }
             } else { console.log("Gamepad press ignored (rate limit)."); }
        }
        lastButtonState = currentButtonState;
        gamepadPollInterval = requestAnimationFrame(pollGamepads); // Continue polling
    }
    gamepadPollInterval = requestAnimationFrame(pollGamepads); // Start the loop
}

/**
 * Stops the gamepad polling loop.
 */
function stopGamepadPolling() {
    if (gamepadPollInterval !== null) {
        cancelAnimationFrame(gamepadPollInterval);
        gamepadPollInterval = null;
        console.log("Stopped gamepad polling.");
    }
}


// --- Utility Functions ---
// Add CSS for scanning highlight (if not already present)
if (!document.getElementById('scanning-styles')) {
    const styleSheet = document.createElement("style");
    styleSheet.id = 'scanning-styles';
    styleSheet.textContent = `
        .scanning { /* Highlight style */
            box-shadow: 0 0 10px 4px #FB4F14 !important; /* Orange glow, !important to override base button shadow */
            outline: none !important; /* Prevent default browser focus outline, !important for specificity */
        }
        .active { /* Optional style for click feedback */
             transform: scale(0.95);
             opacity: 0.8;
        }
        .grid-button-base { /* Ensure base class exists for JS */ }
    `;
    document.head.appendChild(styleSheet);
}

// Add event listener to stop scanning/polling on page unload
window.addEventListener('beforeunload', () => {
    stopAuditoryScanning();
    stopGamepadPolling();
});
